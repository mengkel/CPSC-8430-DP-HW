{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.linspace(0.1, 1, 100)\n",
    "num = np.sin(5*np.pi*X)\n",
    "#den = \n",
    "y = num/(5*np.pi*X)\n",
    "Y = y\n",
    "plt.plot(X,Y)\n",
    "plt.ylabel(r'$sin(5\\pi x)$')\n",
    "plt.xlabel(r'$5\\pi x$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functorch import hessian\n",
    "\n",
    "X_t = torch.tensor(X, dtype = torch.float32)\n",
    "y_t = torch.tensor(Y, dtype = torch.float32)\n",
    "X_t = X_t.view(X_t.shape[0], 1)\n",
    "y_t = y_t.view(y_t.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class func_simulator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(func_simulator, self).__init__()\n",
    "        self.l1 = nn.Linear(1, 10)\n",
    "        self.l2 = nn.Linear(10, 30)\n",
    "        self.l3 = nn.Linear(30, 10)\n",
    "        self.l4 = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.tanh(self.l1(x))\n",
    "        out = F.tanh(self.l2(out))\n",
    "        out = F.tanh(self.l3(out))\n",
    "        out = F.tanh(self.l4(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate,num_epochs    = 0.1,50000\n",
    "model1                      = func_simulator()\n",
    "loss                        = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on batch\n",
    "def train_model(model, learning_rate, num_epochs):\n",
    "    losses = []\n",
    "    gradient                = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "    for epoch in range(num_epochs):\n",
    "        y_pred = model.forward(X_t)\n",
    "        l = loss(y_pred,y_t)\n",
    "        gradient.zero_grad() \n",
    "        #if(epoch+1)%1000==0:\n",
    "#            print(f'epoch:{epoch+1},loss={l.item():.2e}')\n",
    "        losses.append(l.detach().cpu.nympy())\n",
    "        l.backward()\n",
    "        # if epoch >= 35000:\n",
    "        #     lr = 0.5*learning_rate\n",
    "        gradient.step()\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd_lib import autograd_lib\n",
    "model = func_simulator()\n",
    "autograd_lib.register(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_grad_norm(model, criterion, train, target):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    output = model(train)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "\n",
    "    grads = []\n",
    "    for p in model.modules():\n",
    "        if isinstance(p, nn.Linear):\n",
    "            param_norm = p.weight.grad.norm(2).item()\n",
    "            grads.append(param_norm)\n",
    "\n",
    "    grad_mean = np.mean(grads) # compute mean of gradient norms\n",
    "    return grad_mean,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to save activations\n",
    "def save_activations(layer, A, _):\n",
    "\n",
    "    activations[layer] = A\n",
    "\n",
    "# helper function to compute Hessian matrix\n",
    "def compute_hess(layer, _, B):\n",
    "\n",
    "    A = activations[layer]\n",
    "    BA = torch.einsum('nl,ni->nli', B, A) # do batch-wise outer product\n",
    "\n",
    "    # full Hessian\n",
    "    hess[layer] += torch.einsum('nli,nkj->likj', BA, BA) # do batch-wise outer product, then sum over the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_minimum_ratio(model, criterion, train, target):\n",
    "    model.zero_grad()\n",
    "    # compute Hessian matrix\n",
    "    # save the gradient of each layer\n",
    "    with autograd_lib.module_hook(save_activations):\n",
    "        output = model(train)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "    # compute Hessian according to the gradient value stored in the previous step\n",
    "    with autograd_lib.module_hook(compute_hess):\n",
    "        autograd_lib.backward_hessian(output, loss='LeastSquares')\n",
    "\n",
    "    layer_hess = list(hess.values())\n",
    "    minimum_ratio = []\n",
    "\n",
    "    # compute eigenvalues of the Hessian matrix\n",
    "    for h in layer_hess:\n",
    "        size = h.shape[0] * h.shape[1]\n",
    "        h = h.reshape(size, size)\n",
    "        h_eig = torch.symeig(h).eigenvalues # torch.symeig() returns eigenvalues and eigenvectors of a real symmetric matrix\n",
    "        num_greater = torch.sum(h_eig > 0).item()\n",
    "        minimum_ratio.append(num_greater / len(h_eig))\n",
    "\n",
    "    ratio_mean = np.mean(minimum_ratio) # compute mean of minimum ratio\n",
    "\n",
    "    return ratio_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.18249382 0.18126774 0.18004881 0.17883702 0.17763235 0.17643479\n",
      " 0.17524435 0.174061   0.17288476 0.17171562 0.17055354 0.16939852\n",
      " 0.16825058 0.16710968 0.16597582 0.16484898 0.16372913 0.16261633\n",
      " 0.16151048 0.16041163 0.15931973 0.15823479 0.15715677 0.15608566\n",
      " 0.15502146 0.15396413 0.15291367 0.15187009 0.15083332 0.14980339\n",
      " 0.14878024 0.14776386 0.14675426 0.14575139 0.14475524 0.14376578\n",
      " 0.14278302 0.14180689 0.14083742 0.13987455 0.1389183  0.1379686\n",
      " 0.13702546 0.13608883 0.13515873 0.13423511 0.13331793 0.13240719\n",
      " 0.13150285 0.13060492 0.12971334 0.12882809 0.12794916 0.12707649\n",
      " 0.12621011 0.12534995 0.12449602 0.12364824 0.12280662 0.12197113\n",
      " 0.12114174 0.12031842 0.11950117 0.11868992 0.11788466 0.11708537\n",
      " 0.11629202 0.11550459 0.11472303 0.11394732 0.11317741 0.11241333\n",
      " 0.111655   0.11090242 0.11015555 0.10941436 0.10867881 0.10794888\n",
      " 0.10722455 0.1065058  0.10579256 0.10508484 0.10438257 0.10368576\n",
      " 0.1029944  0.10230839 0.10162774 0.10095242 0.10028239 0.09961763\n",
      " 0.09895811 0.0983038  0.09765466 0.09701069 0.09637182 0.09573803\n",
      " 0.09510931 0.09448561 0.09386689 0.09325317]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mengkel/Library/Python/3.9/lib/python/site-packages/torch/nn/functional.py:1956: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "Y_bar, Ep_Loss = train_model(model1, 0.001, 100)\n",
    "Loss = Ep_Loss[:,1]\n",
    "print(Loss)\n",
    "hessian_func = hessian(loss)\n",
    "num_param = sum(p.numel() for p in model1.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'target'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/mengkel/Desktop/course-2022/8430/CPSC-8430-DP-HW/loss_vs_min_ratio.ipynb Cell 7\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mengkel/Desktop/course-2022/8430/CPSC-8430-DP-HW/loss_vs_min_ratio.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m H \u001b[39m=\u001b[39m hessian_func(\u001b[39mtuple\u001b[39;49m(model1\u001b[39m.\u001b[39;49mparameters()))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mengkel/Desktop/course-2022/8430/CPSC-8430-DP-HW/loss_vs_min_ratio.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m H \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([torch\u001b[39m.\u001b[39mcat([e\u001b[39m.\u001b[39mflatten() \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m Hpart]) \u001b[39mfor\u001b[39;00m Hpart \u001b[39min\u001b[39;00m H]) \u001b[39m# flatten\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mengkel/Desktop/course-2022/8430/CPSC-8430-DP-HW/loss_vs_min_ratio.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m H \u001b[39m=\u001b[39m H\u001b[39m.\u001b[39mreshape(num_param, num_param)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/functorch/_src/eager_transforms.py:996\u001b[0m, in \u001b[0;36mjacfwd.<locals>.wrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    993\u001b[0m     _, jvp_out \u001b[39m=\u001b[39m output\n\u001b[1;32m    994\u001b[0m     \u001b[39mreturn\u001b[39;00m jvp_out\n\u001b[0;32m--> 996\u001b[0m results \u001b[39m=\u001b[39m vmap(push_jvp, randomness\u001b[39m=\u001b[39;49mrandomness)(basis)\n\u001b[1;32m    997\u001b[0m \u001b[39mif\u001b[39;00m has_aux:\n\u001b[1;32m    998\u001b[0m     results, aux \u001b[39m=\u001b[39m results\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/functorch/_src/vmap.py:362\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m _check_out_dims_is_int_or_int_pytree(out_dims, func)\n\u001b[1;32m    361\u001b[0m batch_size, flat_in_dims, flat_args, args_spec \u001b[39m=\u001b[39m _process_batched_inputs(in_dims, args, func)\n\u001b[0;32m--> 362\u001b[0m \u001b[39mreturn\u001b[39;00m _flat_vmap(\n\u001b[1;32m    363\u001b[0m     func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    364\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/functorch/_src/vmap.py:35\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfn\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     34\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 35\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/functorch/_src/vmap.py:489\u001b[0m, in \u001b[0;36m_flat_vmap\u001b[0;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     batched_inputs \u001b[39m=\u001b[39m _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)\n\u001b[0;32m--> 489\u001b[0m     batched_outputs \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49mbatched_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    490\u001b[0m     \u001b[39mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n\u001b[1;32m    491\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/functorch/_src/eager_transforms.py:989\u001b[0m, in \u001b[0;36mjacfwd.<locals>.wrapper_fn.<locals>.push_jvp\u001b[0;34m(basis)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpush_jvp\u001b[39m(basis):\n\u001b[0;32m--> 989\u001b[0m     output \u001b[39m=\u001b[39m _jvp_with_argnums(func, args, basis, argnums\u001b[39m=\u001b[39;49margnums, has_aux\u001b[39m=\u001b[39;49mhas_aux)\n\u001b[1;32m    990\u001b[0m     \u001b[39mif\u001b[39;00m has_aux:\n\u001b[1;32m    991\u001b[0m         _, jvp_out, aux \u001b[39m=\u001b[39m output\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/functorch/_src/vmap.py:35\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfn\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     34\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 35\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/functorch/_src/eager_transforms.py:837\u001b[0m, in \u001b[0;36m_jvp_with_argnums\u001b[0;34m(func, primals, tangents, argnums, strict, has_aux)\u001b[0m\n\u001b[1;32m    835\u001b[0m     primals \u001b[39m=\u001b[39m _wrap_all_tensors(primals, level)\n\u001b[1;32m    836\u001b[0m     duals \u001b[39m=\u001b[39m _replace_args(primals, duals, argnums)\n\u001b[0;32m--> 837\u001b[0m result_duals \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49mduals)\n\u001b[1;32m    838\u001b[0m \u001b[39mif\u001b[39;00m has_aux:\n\u001b[1;32m    839\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(result_duals, \u001b[39mtuple\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(result_duals) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/functorch/_src/eager_transforms.py:459\u001b[0m, in \u001b[0;36mjacrev.<locals>.wrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m    458\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper_fn\u001b[39m(\u001b[39m*\u001b[39margs):\n\u001b[0;32m--> 459\u001b[0m     vjp_out \u001b[39m=\u001b[39m _vjp_with_argnums(func, \u001b[39m*\u001b[39;49margs, argnums\u001b[39m=\u001b[39;49margnums, has_aux\u001b[39m=\u001b[39;49mhas_aux)\n\u001b[1;32m    460\u001b[0m     \u001b[39mif\u001b[39;00m has_aux:\n\u001b[1;32m    461\u001b[0m         output, vjp_fn, aux \u001b[39m=\u001b[39m vjp_out\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/functorch/_src/vmap.py:35\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfn\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     34\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 35\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/functorch/_src/eager_transforms.py:289\u001b[0m, in \u001b[0;36m_vjp_with_argnums\u001b[0;34m(func, argnums, has_aux, *primals)\u001b[0m\n\u001b[1;32m    287\u001b[0m     diff_primals \u001b[39m=\u001b[39m _slice_argnums(primals, argnums, as_tuple\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    288\u001b[0m     tree_map_(partial(_create_differentiable, level\u001b[39m=\u001b[39mlevel), diff_primals)\n\u001b[0;32m--> 289\u001b[0m primals_out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49mprimals)\n\u001b[1;32m    291\u001b[0m \u001b[39mif\u001b[39;00m has_aux:\n\u001b[1;32m    292\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(primals_out, \u001b[39mtuple\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(primals_out) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'target'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a792fcb311f9eb9f3c1b942a8c87ada8484712b89b670347c16a1088e0a1f69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
