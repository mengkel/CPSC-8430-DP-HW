{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cdc90b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4670a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#print(device) My laptop uses cpu\n",
    "\n",
    "#Download MNIST dataset in local system\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "train_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = True,                         \n",
    "    transform = ToTensor(), \n",
    "    download = True,            \n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root = 'data', \n",
    "    train = False, \n",
    "    transform = ToTensor()\n",
    ")\n",
    "\n",
    "loaders = {\n",
    "    'train' :   torch.utils.data.DataLoader(train_data,\n",
    "                                            batch_size = 100,\n",
    "                                            shuffle=True\n",
    "                                           ),\n",
    "    'test'  :   torch.utils.data.DataLoader(test_data,\n",
    "                                            batch_size = 100,\n",
    "                                            shuffle=True\n",
    "                                            )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2221640b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model1 (nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(784, 10)\n",
    "        self.l2 = nn.Linear(10, 20)\n",
    "        self.l3 = nn.Linear(20, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.l1(x))\n",
    "        out = F.relu(self.l2(out))\n",
    "        out = self.l3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "328986e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(model, loader, loss_fn): \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    cost_total = 0\n",
    "    counter = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            data, target = batch\n",
    "            output = model(data.view(-1, 784)) # reshape the image (flatten)\n",
    "            cost = loss_fn(output, target)\n",
    "            cost_total += cost\n",
    "            counter += 1\n",
    "            \n",
    "            for i, outputTensor in enumerate(output):\n",
    "                if torch.argmax(outputTensor) == target[i]:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "    return cost_total / counter, round(correct/total, 3)\n",
    "\n",
    "# return the average loss and the accuracy            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4eec3c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def train(num_epochs, model, loaders):\n",
    "    LA = []\n",
    "    model.train()\n",
    "    loss_func = nn.CrossEntropyLoss()  \n",
    "    total_step = len(loaders['train'])\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch +=1\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "            # gives batch data, normalize x when iterate train_loader\n",
    "            b_x = Variable(images)\n",
    "            b_y = Variable(labels)\n",
    "            # or use data, target = Variable(images), Variable(labels)\n",
    "            output = model(b_x.view(-1, 784))\n",
    "            loss = loss_func(output, b_y)\n",
    "\n",
    "            # clear gradients for this training step\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # backpropagation, compute gradients\n",
    "            loss.backward()\n",
    "\n",
    "            #apply gradients\n",
    "            optimizer.step()\n",
    "            \n",
    "            temp_df = pd.DataFrame()\n",
    "            for name, parameter in model.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    weights = torch.nn.utils.parameters_to_vector(parameter).detach().numpy() \n",
    "                    temp_df = pd.concat([temp_df, pd.DataFrame(weights).T], axis = 1)\n",
    "                \n",
    "            df = pd.concat([df, temp_df], axis = 0)\n",
    "            train_loss, train_acc = calculate_loss(M, train_loader, loss_fn)\n",
    "            test_loss, test_acc = calculate_loss(M, test_loader, loss_fn)\n",
    "\n",
    "            train_loss_arr.append(train_loss)\n",
    "            test_loss_arr.append(test_loss)\n",
    "            train_acc_arr.append(train_acc)\n",
    "            test_acc_arr.append(test_acc)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be72d201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_batch_size = 1000\n",
    "# test_batch_size = 1000\n",
    "train_loader, test_loader = loaders['train'], loaders['test']\n",
    "train_loss_arr = []\n",
    "test_loss_arr = []\n",
    "train_acc_arr = []\n",
    "test_acc_arr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bf479c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(M\u001b[38;5;241m.\u001b[39mparameters(),lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0004\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[1;32m     12\u001b[0m model_name1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTimes: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(count)    \n\u001b[0;32m---> 13\u001b[0m temp_df \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m all_df \u001b[38;5;241m=\u001b[39m all_df\u001b[38;5;241m.\u001b[39mappend(temp_df)\n",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(num_epochs, model, loaders)\u001b[0m\n\u001b[1;32m     34\u001b[0m         temp_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([temp_df, pd\u001b[38;5;241m.\u001b[39mDataFrame(weights)\u001b[38;5;241m.\u001b[39mT], axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     36\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df, temp_df], axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m test_loss, test_acc \u001b[38;5;241m=\u001b[39m calculate_loss(M, test_loader, loss_fn)\n\u001b[1;32m     40\u001b[0m train_loss_arr\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mcalculate_loss\u001b[0;34m(model, loader, loss_fn)\u001b[0m\n\u001b[1;32m      5\u001b[0m counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m      8\u001b[0m         data, target \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m      9\u001b[0m         output \u001b[38;5;241m=\u001b[39m model(data\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m784\u001b[39m)) \u001b[38;5;66;03m# reshape the image (flatten)\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torchvision/datasets/mnist.py:142\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    138\u001b[0m img, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[index], \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargets[index])\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# doing this so that it is consistent with all other datasets\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# to return a PIL Image\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "max_epochs = 5\n",
    "all_df = pd.DataFrame()\n",
    "columns=[\"x\",\"y\",\"Times\"]\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for count in range(8):\n",
    "    print(\"Time: \"+str(count))\n",
    "    M = Model1()\n",
    "    optimizer = torch.optim.Adam(M.parameters(),lr = 0.0004, weight_decay=1e-4)\n",
    "    model_name1 = \"Times: \"+str(count)    \n",
    "    temp_df = train(max_epochs, M, loaders)\n",
    "\n",
    "    all_df = all_df.append(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd9d8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = all_df\n",
    "df = np.array(df)\n",
    "pca = PCA(n_components=2)\n",
    "new_data = pca.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36a2f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(new_data, columns=['x','y'])\n",
    "df['Accuracy'] = train_acc_arr\n",
    "df['Loss'] = train_loss_arr\n",
    "final_df = df.iloc[::3, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b5aed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(120):\n",
    "    m = list(final_df['Accuracy'])[i]\n",
    "    plt.scatter(final_df['x'][i*3], final_df['y'][i*3], marker = f'${m}$')\n",
    "    plt.title(\"PCA for model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
